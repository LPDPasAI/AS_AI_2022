{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Accesso negato: 'C:\\\\Users\\\\lpdepersiis\\\\Anaconda3\\\\Lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\lpdepersiis\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q -U \"tensorflow-text==2.8.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\lpdepersiis\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tf-models-official==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url, untar=True, cache_dir='.', cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b\"This flick reminds me some really bad science-fiction movies from 50's and 60's.It is not scary or interesting,but it's dull,cheesy and stupid.Special effects are laughable,all actors are ludicrous and the ending is simply awful.Don't waste your money,rent or buy something better.I give it 3.5 out of 10( I found this turkey quite amusing because of its stupidity).\"\n",
      "Label : 0 (neg)\n",
      "Review: b'Finally a thriller which omits the car chases, explosions and other eye catching effects. The movie combines a simple plot (assasination of a french president) with an excellent background. It takes a look behind mans behavior with authorities, and explains why we would obey almost every order (even murder) which would be given to us.<br /><br />Furthermore it shows us how secret services can manipulate the run of history and how hardly they can be controlled. The best thing on this movie is, that there is no classic \"Hollywood end\" which can easily be predicted.'\n",
      "Label : 1 (pos)\n",
      "Review: b\"I'm a Christian who generally believes in the theology taught in Left Behind. That being said, I think Left Behind is one of the worst films I've seen in some time.<br /><br />To have a good movie, you need to have a well-written screenplay. Left Behind fell woefully short on this. For one thing, it radically deviates from the book. Sometimes this is done to condense a 400-page novel down to a two-hour film, but in this film I saw changes that made no sense whatsoever.<br /><br />Another thing, there is zero character development. When characters in the story get saved (I won't say who), the book makes it clear that it's a long, soul-searching process. In the film it's quick and artificial. The book is written decently enough where people like Rayford Steele, Buck Williams and Hattie Durham seem real, but in the movie scenarios are consistently given the quick treatment without anything substantial. In another scene where one character gets angry about being left behind (again, I won't say who), it seems artificial.<br /><br />I realize as a Christian it's unedifying for me to say I disliked this film, but I can't in a good conscience recommend a film that I feel was horribly done. Perhaps it would've been better to make the first book into 2-3 films. Either way, Christians need to realize that to be taken seriously as filmmakers, we need to start by putting together a film in a quality way. I realize a lot of effort probably went into Left Behind, but that's the way I see it.\"\n",
      "Label : 0 (neg)\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(5, 8):\n",
    "        print(f'Review: {text_batch.numpy()[i]}')\n",
    "        label = label_batch.numpy()[i]\n",
    "        print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_mask', 'input_type_ids', 'input_word_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['this is such an amazing movie!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Pooled Outputs Shape:(1, 512)\n",
      "Pooled Outputs Values:[ 0.76262885  0.99280983 -0.18611872  0.3667386   0.15233792  0.65504557\n",
      "  0.9681154  -0.948627    0.00216077 -0.9877732   0.06842785 -0.9763061 ]\n",
      "Sequence Outputs Shape:(1, 128, 512)\n",
      "Sequence Outputs Values:[[-0.28946352  0.34321296  0.3323149  ...  0.2130083   0.71020764\n",
      "  -0.05771133]\n",
      " [-0.2874205   0.31981108 -0.2301845  ...  0.5845506  -0.2132971\n",
      "   0.7269202 ]\n",
      " [-0.6615705   0.6887687  -0.8743301  ...  0.10877223 -0.26173192\n",
      "   0.47855487]\n",
      " ...\n",
      " [-0.22561216 -0.2892565  -0.07064489 ...  0.4756605   0.832772\n",
      "   0.40025362]\n",
      " [-0.2982421  -0.2747309  -0.05450581 ...  0.48849702  1.095536\n",
      "   0.18163332]\n",
      " [-0.44378197  0.00930689  0.07223699 ...  0.17290227  1.1833242\n",
      "   0.07898021]]\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In layer  word_embeddings/embeddings:0\n",
      "In layer  position_embedding/embeddings:0\n",
      "In layer  type_embeddings/embeddings:0\n",
      "In layer  embeddings/layer_norm/gamma:0\n",
      "In layer  embeddings/layer_norm/beta:0\n",
      "In layer  transformer/layer_0/self_attention/query/kernel:0\n",
      "In layer  transformer/layer_0/self_attention/query/bias:0\n",
      "In layer  transformer/layer_0/self_attention/key/kernel:0\n",
      "In layer  transformer/layer_0/self_attention/key/bias:0\n",
      "In layer  transformer/layer_0/self_attention/value/kernel:0\n",
      "In layer  transformer/layer_0/self_attention/value/bias:0\n",
      "In layer  transformer/layer_0/self_attention/attention_output/kernel:0\n",
      "In layer  transformer/layer_0/self_attention/attention_output/bias:0\n",
      "In layer  transformer/layer_0/self_attention_layer_norm/gamma:0\n",
      "In layer  transformer/layer_0/self_attention_layer_norm/beta:0\n",
      "In layer  transformer/layer_0/intermediate/kernel:0\n",
      "In layer  transformer/layer_0/intermediate/bias:0\n",
      "In layer  transformer/layer_0/output/kernel:0\n",
      "In layer  transformer/layer_0/output/bias:0\n",
      "In layer  transformer/layer_0/output_layer_norm/gamma:0\n",
      "In layer  transformer/layer_0/output_layer_norm/beta:0\n",
      "In layer  transformer/layer_1/self_attention/query/kernel:0\n",
      "In layer  transformer/layer_1/self_attention/query/bias:0\n",
      "In layer  transformer/layer_1/self_attention/key/kernel:0\n",
      "In layer  transformer/layer_1/self_attention/key/bias:0\n",
      "In layer  transformer/layer_1/self_attention/value/kernel:0\n",
      "In layer  transformer/layer_1/self_attention/value/bias:0\n",
      "In layer  transformer/layer_1/self_attention/attention_output/kernel:0\n",
      "In layer  transformer/layer_1/self_attention/attention_output/bias:0\n",
      "In layer  transformer/layer_1/self_attention_layer_norm/gamma:0\n",
      "In layer  transformer/layer_1/self_attention_layer_norm/beta:0\n",
      "In layer  transformer/layer_1/intermediate/kernel:0\n",
      "In layer  transformer/layer_1/intermediate/bias:0\n",
      "In layer  transformer/layer_1/output/kernel:0\n",
      "In layer  transformer/layer_1/output/bias:0\n",
      "In layer  transformer/layer_1/output_layer_norm/gamma:0\n",
      "In layer  transformer/layer_1/output_layer_norm/beta:0\n",
      "In layer  transformer/layer_2/self_attention/query/kernel:0\n",
      "In layer  transformer/layer_2/self_attention/query/bias:0\n",
      "In layer  transformer/layer_2/self_attention/key/kernel:0\n",
      "In layer  transformer/layer_2/self_attention/key/bias:0\n",
      "In layer  transformer/layer_2/self_attention/value/kernel:0\n",
      "In layer  transformer/layer_2/self_attention/value/bias:0\n",
      "In layer  transformer/layer_2/self_attention/attention_output/kernel:0\n",
      "In layer  transformer/layer_2/self_attention/attention_output/bias:0\n",
      "In layer  transformer/layer_2/self_attention_layer_norm/gamma:0\n",
      "In layer  transformer/layer_2/self_attention_layer_norm/beta:0\n",
      "In layer  transformer/layer_2/intermediate/kernel:0\n",
      "In layer  transformer/layer_2/intermediate/bias:0\n",
      "In layer  transformer/layer_2/output/kernel:0\n",
      "In layer  transformer/layer_2/output/bias:0\n",
      "In layer  transformer/layer_2/output_layer_norm/gamma:0\n",
      "In layer  transformer/layer_2/output_layer_norm/beta:0\n",
      "In layer  transformer/layer_3/self_attention/query/kernel:0\n",
      "In layer  transformer/layer_3/self_attention/query/bias:0\n",
      "In layer  transformer/layer_3/self_attention/key/kernel:0\n",
      "In layer  transformer/layer_3/self_attention/key/bias:0\n",
      "In layer  transformer/layer_3/self_attention/value/kernel:0\n",
      "In layer  transformer/layer_3/self_attention/value/bias:0\n",
      "In layer  transformer/layer_3/self_attention/attention_output/kernel:0\n",
      "In layer  transformer/layer_3/self_attention/attention_output/bias:0\n",
      "In layer  transformer/layer_3/self_attention_layer_norm/gamma:0\n",
      "In layer  transformer/layer_3/self_attention_layer_norm/beta:0\n",
      "In layer  transformer/layer_3/intermediate/kernel:0\n",
      "In layer  transformer/layer_3/intermediate/bias:0\n",
      "In layer  transformer/layer_3/output/kernel:0\n",
      "In layer  transformer/layer_3/output/bias:0\n",
      "In layer  transformer/layer_3/output_layer_norm/gamma:0\n",
      "In layer  transformer/layer_3/output_layer_norm/beta:0\n",
      "In layer  pooler_transform/kernel:0\n",
      "In layer  pooler_transform/bias:0\n",
      "In layer  Variable:0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bert_model.weights)):\n",
    "    print(\"In layer \", bert_model.weights[i].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The name \"classifier\" is used 3 times in the model. All layer names should be unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e840f3055c3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'classifier'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mclassifier_model\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0m\u001b[0;32m    230\u001b[0m         self.inputs, self.outputs)\n\u001b[0;32m    231\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mall_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m       raise ValueError(\n\u001b[0m\u001b[0;32m   1050\u001b[0m           \u001b[1;34mf'The name \"{name}\" is used {all_names.count(name)} '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           'times in the model. All layer names should be unique.')\n",
      "\u001b[1;31mValueError\u001b[0m: The name \"classifier\" is used 3 times in the model. All layer names should be unique."
     ]
    }
   ],
   "source": [
    "# build classifier model\n",
    "\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=False, name='BERT_encoder')  # , trainable=False\n",
    "outputs = encoder(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "net = tf.keras.layers.Dense(128, activation=None, name='Dense_1')(net)\n",
    "net = tf.keras.layers.Dropout(0.4)(net)\n",
    "net = tf.keras.layers.Dense(64, activation=None, name='Dense_2')(net)\n",
    "net = tf.keras.layers.Dropout(0.1)(net)\n",
    "\n",
    "net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "\n",
    "classifier_model =  tf.keras.Model(text_input, net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " preprocessing (KerasLayer)     {'input_mask': (Non  0           ['text[0][0]']                   \n",
      "                                e, 128),                                                          \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " BERT_encoder (KerasLayer)      {'default': (None,   28763649    ['preprocessing[0][0]',          \n",
      "                                512),                             'preprocessing[0][1]',          \n",
      "                                 'encoder_outputs':               'preprocessing[0][2]']          \n",
      "                                 [(None, 128, 512),                                               \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 (None, 128, 512)],                                               \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 512),                                                       \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 512)}                                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['BERT_encoder[0][5]']           \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 1)            513         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,764,162\n",
      "Trainable params: 28,764,161\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is such an amazing movie!']\n",
      "tf.Tensor([[0.74897844]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(text_test)\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 5088s 8s/step - loss: 0.4897 - binary_accuracy: 0.7413 - val_loss: 0.3736 - val_binary_accuracy: 0.8252\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 5392s 9s/step - loss: 0.3267 - binary_accuracy: 0.8553 - val_loss: 0.3633 - val_binary_accuracy: 0.8484\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 5654s 9s/step - loss: 0.2478 - binary_accuracy: 0.8960 - val_loss: 0.3993 - val_binary_accuracy: 0.8484\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 5698s 9s/step - loss: 0.1920 - binary_accuracy: 0.9264 - val_loss: 0.4358 - val_binary_accuracy: 0.8470\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 5685s 9s/step - loss: 0.1508 - binary_accuracy: 0.9438 - val_loss: 0.4729 - val_binary_accuracy: 0.8524\n"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    history = classifier_model.fit(x=train_ds,\n",
    "                                   validation_data=val_ds,\n",
    "                                   epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 5814s 9s/step - loss: 0.1359 - binary_accuracy: 0.9488 - val_loss: 0.4729 - val_binary_accuracy: 0.8524\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 5782s 9s/step - loss: 0.1367 - binary_accuracy: 0.9474 - val_loss: 0.4729 - val_binary_accuracy: 0.8524\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 8176s 13s/step - loss: 0.1380 - binary_accuracy: 0.9473 - val_loss: 0.4729 - val_binary_accuracy: 0.8524\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 6083s 10s/step - loss: 0.1365 - binary_accuracy: 0.9495 - val_loss: 0.4729 - val_binary_accuracy: 0.8524\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 5788s 9s/step - loss: 0.1375 - binary_accuracy: 0.9498 - val_loss: 0.4729 - val_binary_accuracy: 0.8524\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    history = classifier_model.fit(x=train_ds,\n",
    "                                   validation_data=val_ds,\n",
    "                                   epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
