{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collect and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\"sport\", \"politics\", \"economics\", \"psychology\", \"philosophy\", \"literature\", \"physics\"]\n",
    "DATASET_PATH = \"./dataset/\"\n",
    "SENTENCE_LENGTH = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pag = wikipedia.page('turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pag = pag.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pag = re.sub('===', ' ', pag)\n",
    "pag = re.sub('==', ' ', pag)\n",
    "pag = re.sub('=', ' ', pag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_or_create_dir(directory):\n",
    "    if not os.path.isdir(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(\"directory\", directory, \"created\")\n",
    "    else:\n",
    "        print(\"directory\", directory, \"exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_art(category):\n",
    "    #for cat in category:\n",
    "    cat = category\n",
    "    ric = wikipedia.search(cat)\n",
    "    dir_name = DATASET_PATH + cat + \"/\"\n",
    "    verify_or_create_dir(dir_name)\n",
    "    for art in ric:\n",
    "        try:\n",
    "            pag = wikipedia.page(art)\n",
    "            pag = pag.content\n",
    "            pag = re.sub('===', ' ', pag)\n",
    "            pag = re.sub('==', ' ', pag)\n",
    "            pag = re.sub('=', ' ', pag)\n",
    "            with open(dir_name + art + \".txt\", 'w', encoding='UTF-8') as file:\n",
    "                file.write(pag)\n",
    "        except:\n",
    "            print(\"Error in \", art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories(cats):\n",
    "    for cat in cats:\n",
    "        get_art(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ./dataset/sport/ created\n",
      "Error in  Sport Chek\n",
      "Error in  Swimming (sport)\n",
      "directory ./dataset/politics/ created\n",
      "directory ./dataset/economics/ created\n",
      "Error in  Socioeconomics\n",
      "Error in  Bachelor of Economics\n",
      "directory ./dataset/psychology/ created\n",
      "directory ./dataset/philosophy/ created\n",
      "Error in  Indian philosophy\n",
      "directory ./dataset/literature/ created\n",
      "directory ./dataset/physics/ created\n",
      "Error in  Physics\n"
     ]
    }
   ],
   "source": [
    "get_categories(CATEGORIES)   # get_categories(CATEGORIES[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(categories, name_file):\n",
    "    count = 0\n",
    "    with open(DATASET_PATH + name_file + \".csv\", 'w', newline='', encoding='UTF-8') as file_out:\n",
    "        writer = csv.writer(file_out, delimiter= ',')\n",
    "        for idx, cat in tqdm(enumerate(categories)):\n",
    "            print(\"\\n *** Importing\", cat)\n",
    "            path_cat = DATASET_PATH + cat + \"/\"\n",
    "            files = os.listdir(path_cat)\n",
    "            print(files)\n",
    "            for f in files:\n",
    "                with open(path_cat + f, 'r', encoding='UTF-8') as file_in:\n",
    "                    file_text = file_in.read()\n",
    "                    file_text = re.sub(\"\\n\", \" \", file_text)\n",
    "                    lst_sentences = file_text.split('. ')\n",
    "                    row = ''\n",
    "                    row_len = 0\n",
    "                    for sentence in lst_sentences:\n",
    "                        len_sent = len(sentence.split())\n",
    "                        row += sentence\n",
    "                        row_len += len_sent\n",
    "                        if row_len + 9 > SENTENCE_LENGTH:\n",
    "                            count += 1\n",
    "                            writer.writerow([row, idx])\n",
    "                            row = ''\n",
    "                            row_len = 0\n",
    "            print(count)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3964828eb27f4ba89bb624f275e48d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** Importing sport\n",
      "['BBC Sport.txt', 'Bugatti Chiron.txt', 'Rowing (sport).txt', 'Sport (US magazine).txt', 'Sport diving (sport).txt', 'Sport of athletics.txt', 'Sport, Sport, Sport.txt', 'Sport.txt']\n",
      "307\n",
      "\n",
      " *** Importing politics\n",
      "['Ireland.txt', 'Political moderate.txt', 'Political party.txt', 'Political science.txt', 'Political spectrum.txt', 'Politics (Aristotle).txt', 'Politics of India.txt', 'Politics of the United States.txt', 'Politics.txt', 'Right-wing politics.txt']\n",
      "758\n",
      "\n",
      " *** Importing economics\n",
      "['Behavioral economics.txt', 'Economics.txt', 'Keynesian economics.txt', 'Labour economics.txt', 'London School of Economics.txt', 'Managerial economics.txt', 'Positive economics.txt', 'Service (economics).txt']\n",
      "1174\n",
      "\n",
      " *** Importing psychology\n",
      "['AP Psychology.txt', 'Cognitive psychology.txt', 'Developmental psychology.txt', 'Evolutionary psychology.txt', 'Flow (psychology).txt', 'Gestalt psychology.txt', 'Psyche (psychology).txt', 'Psychology.txt', 'Social psychology.txt', 'Transpersonal psychology.txt']\n",
      "1930\n",
      "\n",
      " *** Importing philosophy\n",
      "['Cynicism (philosophy).txt', 'Doctor of Philosophy.txt', 'Hindu philosophy.txt', 'Philosophy of science.txt', 'Philosophy.txt', 'Political philosophy.txt', \"The Philosophy of 'As if'.txt\", 'Western philosophy.txt', 'Will (philosophy).txt']\n",
      "2532\n",
      "\n",
      " *** Importing literature\n",
      "['Erotic literature.txt', 'Grey literature.txt', 'History of literature.txt', 'Literature review.txt', 'Literature.txt', 'Nobel Prize in Literature.txt', 'Philippine literature.txt', 'Victorian literature.txt', 'Western literature.txt', 'World literature.txt']\n",
      "2892\n",
      "\n",
      " *** Importing physics\n",
      "['Glossary of physics.txt', 'Nobel Prize in Physics.txt', 'Nuclear physics.txt', 'Particle physics.txt', 'Plasma (physics).txt', 'Power (physics).txt', 'Quantum mechanics.txt', 'Theoretical physics.txt', 'Work (physics).txt']\n",
      "3341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_dataset(CATEGORIES, \"dataset_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_removal(text):\n",
    "    text = re.sub('@', '', text)    \n",
    "    text = re.sub('#', '', text)    \n",
    "    text = re.sub('°', '', text)\n",
    "    #text = re.sub('<[/A-Za-z]*>', '', text)\n",
    "    text = re.sub('\\...', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(text):\n",
    "    text = text.replace(\":-)\", \"smile\")\n",
    "    text = text.replace(\":)\", \"smile\")\n",
    "    text = text.replace(\":D\", \"smile\")\n",
    "    text = text.replace(\"b4\", \"before\")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\"   \", \" \")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = text.replace(\"  \", \" \")    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lpdepersiis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal(text):\n",
    "    new_words = []\n",
    "    for word in text:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    #print(text)\n",
    "    text = text.lower()\n",
    "    #print(text)\n",
    "    text = normalization(text)\n",
    "    #print(text)\n",
    "    text = noise_removal(text)\n",
    "    #print(text)\n",
    "    text = text.split(' ')\n",
    "    #text = nltk.word_tokenize(text)\n",
    "    #print(text)\n",
    "    text = stopword_removal(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"asfdaf, Risultati immagini per bloomberg health care efficiency °°° # The Bloomberg Health-Efficiency Index, first conducted in 2013, tracks life expectancy and medical spending to determine which health-care systems have the best outcomes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(noise_removal(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(pos, len_vect):\n",
    "    vec = np.zeros(len_vect)\n",
    "    vec[int(pos)] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(get_one_hot(2, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y_preprocessed(name_file):\n",
    "    x_rec = []\n",
    "    y_rec = []\n",
    "    with open(DATASET_PATH + name_file + \".csv\", 'r', encoding='UTF-8') as file_dtset:\n",
    "        reader = csv.reader(file_dtset, delimiter= ',')\n",
    "        lines = [line for line in reader]\n",
    "        #header = next(lines)\n",
    "        # pbar = tqdm(lines)\n",
    "        for row in tqdm(lines, total=len(lines)):\n",
    "            try:\n",
    "                text = row[0]\n",
    "                text = preprocess_text(text)\n",
    "                x_rec.append(text)\n",
    "                y_rec.append(get_one_hot(row[1], len(CATEGORIES)))\n",
    "                #print(type(y_rec))\n",
    "                #print(type(x_rec))\n",
    "            except:\n",
    "                print(\"Exception:\", row[0], row[1])\n",
    "        y_rec = np.array(y_rec)\n",
    "        \n",
    "    return x_rec, y_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1458a099afe3473fa5839afce2481125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3341.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_rec, y_rec = get_x_y_preprocessed(\"dataset_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_rec))\n",
    "print(y_rec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
